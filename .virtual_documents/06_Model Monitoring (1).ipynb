


import os, sys, warnings
import numpy as np
import pandas as pd
import h2o

from sklearn.metrics import (
    roc_auc_score, average_precision_score, classification_report,
    confusion_matrix, brier_score_loss
)

warnings.filterwarnings("ignore", category=UserWarning)

try:
    import evidently
    from evidently.report import Report
    from evidently import metrics as evm
    # handle 0.4.x vs 0.7.x naming
    DatasetDriftMetric  = getattr(evm, "DatasetDriftMetric", None)
    DatasetMissingValuesMetric = getattr(evm, "DatasetMissingValuesMetric", None)
    ColumnSummaryMetric = getattr(evm, "ColumnSummaryMetric", None)
    BinaryClassificationQualityMetric = (
        getattr(evm, "BinaryClassificationQualityMetric", None)
        or getattr(evm, "ClassificationPerformanceMetric", None)
    )
    print("Evidently:", getattr(evidently, "__version__", "(unknown)"))
except Exception as e:
    DatasetDriftMetric = DatasetMissingValuesMetric = ColumnSummaryMetric = BinaryClassificationQualityMetric = None
    print("Evidently not available -> reports will be skipped:", e)






ROOT        = "data/processed"
PATH_TRAIN  = f"{ROOT}/train_data.csv"   # processed CSVs from your 02 notebook
PATH_TEST   = f"{ROOT}/test_data.csv"

MODEL_ID    = "XGBoost_3_AutoML_1_20250820_184414"  # your screenshot / cluster id
TARGET_COL  = "reordered"
ID_COL      = "order_id"
CATEGORICAL_FORCE = ["aisle_id", "department_id", TARGET_COL]  # treated as H2O factors

# keep notebook responsive; raise/remove once stable
REF_MAX = 300_000
CUR_MAX = 200_000

# engineered feature names used in training
ENGINEERED = [
    "times_bought_by_user",
    "avg_user_product_position",
    "last_order_number",
    "num_orders",
    "avg_days_since_prior_order",
    "num_items",
    "user_reorder_prop",
    "product_total_orders",
    "product_reorder_prop",
    "avg_add_to_cart_order",
]





try:
    import h2o
    h2o.cluster().shutdown(prompt=False)
except Exception:
    pass

import h2o
h2o.init(ip="localhost", port=54329, start_h2o=True, nthreads=-1, max_mem_size="8G")
print("H2O server:", h2o.cluster().version)


import os, h2o
MODEL_ID = os.path.basename("models/XGBoost_grid_1_AutoML_1_20250818_235405_model_3")

try:
    model = h2o.get_model(MODEL_ID)   
    print("Found in cluster:", model.model_id)
except Exception:
    model = None
    print("Model not present in this H2O session:", MODEL_ID)






def load_df(path, n=None):
    df = pd.read_csv(path)
    if n is not None and len(df) > n:
        df = df.sample(n, random_state=42).reset_index(drop=True)
    return df

def ensure_object_cats(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    # stringify potential categoricals
    for c in df.columns:
        if df[c].dtype == "object" or str(df[c].dtype).startswith("category"):
            df[c] = df[c].astype(str)
    for c in CATEGORICAL_FORCE:
        if c in df.columns and df[c].dtype != "object":
            df[c] = df[c].astype(str)
    return df






assert os.path.exists(PATH_TRAIN) and os.path.exists(PATH_TEST), "Processed train/test CSVs not found."
ref_full = load_df(PATH_TRAIN, REF_MAX)
cur_full = load_df(PATH_TEST,  CUR_MAX)

# drop extras to match your modeling notebook
for df in (ref_full, cur_full):
    df.drop(columns=["Unnamed: 0", "product_name"], errors="ignore", inplace=True)
    # IMPORTANT: keep 'aisle_id' & 'department_id' (categoricals) and the ENGINEERED cols
    # Any missing engineered cols -> create (rare if CSVs are the latest)
    for c in ENGINEERED:
        if c not in df.columns:
            df[c] = np.nan

ref_full = ensure_object_cats(ref_full)
cur_full = ensure_object_cats(cur_full)

print("Frames:", ref_full.shape, cur_full.shape)





def expected_features_no_target(h2o_model, target=TARGET_COL):
    out = h2o_model._model_json.get("output", {}) or {}
    names = out.get("names", []) or []
    resp  = out.get("response_column_name", None)
    drop = {target, resp, "predict", "p0", "p1", None}
    return [c for c in names if c not in drop]

def coerce_to_expected(df: pd.DataFrame, exp_cols, cat_cols=("aisle_id","department_id")) -> pd.DataFrame:
    df = df.copy()
    df.columns = [c.strip() for c in df.columns]  # normalize names
    # add any missing expected columns (cats -> '0', nums -> 0.0)
    missing = [c for c in exp_cols if c not in df.columns]
    for c in missing:
        if c in cat_cols:
            df[c] = "0"
        else:
            df[c] = 0.0
    # order strictly to expected features (target kept outside)
    return df[exp_cols + [c for c in df.columns if c not in exp_cols]]

def median_fill_expected(train_df: pd.DataFrame, test_df: pd.DataFrame, exp_cols, cat_cols=("aisle_id","department_id")):
    num_exp = [c for c in exp_cols if c not in cat_cols]
    med = pd.to_numeric(train_df[num_exp].stack(), errors="coerce").groupby(level=1).median()
    for c in num_exp:
        train_df[c] = pd.to_numeric(train_df[c], errors="coerce").fillna(med.get(c, 0.0))
        test_df[c]  = pd.to_numeric(test_df[c],  errors="coerce").fillna(med.get(c, 0.0))
    return train_df, test_df

exp = expected_features_no_target(model)  # <-- no target column here
ref_full = coerce_to_expected(ref_full, exp)
cur_full = coerce_to_expected(cur_full, exp)
ref_full, cur_full = median_fill_expected(ref_full, cur_full, exp)

print("Missing vs model (train):", [c for c in exp if c not in ref_full.columns])
print("Missing vs model (test) :", [c for c in exp if c not in cur_full.columns])





# FIXED_THRESHOLD = 0.5

# def enforce_factor_cols(hf, cols):
#     for c in cols:
#         if c in hf.columns:
#             hf[c] = hf[c].asfactor()

# def score_and_metrics_features(df_features: pd.DataFrame, df_with_target: pd.DataFrame, h2o_model, threshold=0.7448951904827723):
#     # H2OFrame with FEATURES ONLY (prevents duplicate target parsing)
#     hf = h2o.H2OFrame(df_features[exp])
#     enforce_factor_cols(hf, CATEGORICAL_FORCE)

#     pred = h2o_model.predict(hf).as_data_frame(use_multi_thread=True)
#     p1   = pred["p1"].values if "p1" in pred.columns else pred.iloc[:, -1].values

#     y = (df_with_target[TARGET_COL].astype(str).values == "1").astype(int)
#     yhat = (p1 >= threshold).astype(int)

#     scored = pd.DataFrame(index=df_with_target.index)
#     if ID_COL in df_with_target.columns:
#         scored[ID_COL] = df_with_target[ID_COL].values
#     scored["proba"]    = p1
#     scored["y_pred"]   = yhat
#     scored[TARGET_COL] = y

#     metrics = dict(
#         auc   = roc_auc_score(y, p1),
#         ap    = average_precision_score(y, p1),
#         brier = brier_score_loss(y, p1),
#         report= classification_report(y, yhat, digits=4),
#         cm    = confusion_matrix(y, yhat),
#     )
#     return scored, metrics

# # We need the target column alongside features for metrics.
# # Your processed CSVs include TARGET_COL; we align dataframes by index:
# ref_with_target = load_df(PATH_TRAIN, REF_MAX)
# cur_with_target = load_df(PATH_TEST,  CUR_MAX)
# for df in (ref_with_target, cur_with_target):
#     df = df  # (placeholder to keep structure similar)
#     # ensure target dtype consistent
#     if TARGET_COL in df.columns and df[TARGET_COL].dtype != "object":
#         df[TARGET_COL] = df[TARGET_COL].astype(int)

# ref_scored, m_ref = score_and_metrics_features(ref_full, ref_with_target, model, FIXED_THRESHOLD)
# cur_scored, m_cur = score_and_metrics_features(cur_full, cur_with_target, model, FIXED_THRESHOLD)

# print("REF  -> AUC: %.4f | PR-AUC: %.4f | Brier: %.4f" % (m_ref["auc"], m_ref["ap"], m_ref["brier"]))
# print("CUR  -> AUC: %.4f | PR-AUC: %.4f | Brier: %.4f" % (m_cur["auc"], m_cur["ap"], m_cur["brier"]))
# print("\nREF @thr report:\n", m_ref["report"])
# print("\nCUR @thr report:\n", m_cur["report"])





def build_report(ref_df, cur_df, out_html):
    if not (Report and (DatasetDriftMetric or DatasetMissingValuesMetric)):
        print("Evidently is not installed/compatible; skipping report.")
        return
    metrics = []
    if DatasetDriftMetric:          metrics.append(DatasetDriftMetric())
    if DatasetMissingValuesMetric:  metrics.append(DatasetMissingValuesMetric())
    if ColumnSummaryMetric:         metrics.append(ColumnSummaryMetric(column_name="proba"))
    if BinaryClassificationQualityMetric:
        # some versions want 'prediction' (pred label), others support proba via params
        metrics.append(BinaryClassificationQualityMetric(target=TARGET_COL, prediction="y_pred"))
    rep = Report(metrics=metrics)
    rep.run(reference_data=ref_df, current_data=cur_df)
    rep.save_html(out_html)
    print("Saved:", out_html)

build_report(ref_scored, cur_scored, "baseline_vs_test.html")





# =========================================================

# =========================================================
changed = cur_full.copy()

if 'user_reorder_prop' in load_df(PATH_TEST, 5).columns:  # peek columns safely
    base = load_df(PATH_TEST, CUR_MAX)
    if 'user_reorder_prop' in base.columns:
        reorder_count = pd.to_numeric(base['user_reorder_prop']*base['num_items'])
        base['user_reorder_prop'] = reorder_count
        # reflect into 'changed' if that feature is in exp (it usually isn't), safe no-op otherwise

# second tweak: bump user-product frequency by +2 (if present)
if "times_bought_by_user" in changed.columns:
    changed["times_bought_by_user"] = pd.to_numeric(changed["times_bought_by_user"], errors="coerce").fillna(0) + 2

chg_with_target = load_df(PATH_TEST, CUR_MAX)
chg_scored, m_chg = score_and_metrics_features(changed, chg_with_target, model, FIXED_THRESHOLD)
print("\nCHGD -> AUC: %.4f | PR-AUC: %.4f | Brier: %.4f" % (m_chg["auc"], m_chg["ap"], m_chg["brier"]))
print("\nCHGD @thr report:\n", m_chg["report"])
build_report(ref_scored, chg_scored, "baseline_vs_changed.html")



